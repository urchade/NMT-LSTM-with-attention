{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Neural Machine Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: Download and unzip the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-04-15 21:59:03--  http://www.manythings.org/anki/fra-eng.zip\n",
      "Résolution de www.manythings.org (www.manythings.org)… 172.67.173.198, 104.21.55.222, 2606:4700:3036::ac43:adc6, ...\n",
      "Connexion à www.manythings.org (www.manythings.org)|172.67.173.198|:80… connecté.\n",
      "requête HTTP transmise, en attente de la réponse… 200 OK\n",
      "Taille : 6281268 (6,0M) [application/zip]\n",
      "Sauvegarde en : « fra-eng.zip.1 »\n",
      "\n",
      "fra-eng.zip.1       100%[===================>]   5,99M  --.-KB/s    ds 0,05s   \n",
      "\n",
      "2021-04-15 21:59:05 (112 MB/s) — « fra-eng.zip.1 » sauvegardé [6281268/6281268]\n",
      "\n",
      "Archive:  fra-eng.zip\n",
      "replace _about.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: ^C\n"
     ]
    }
   ],
   "source": [
    "# see http://www.manythings.org/anki\n",
    "\n",
    "! wget http://www.manythings.org/anki/fra-eng.zip\n",
    "    \n",
    "! unzip fra-eng.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Reading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('fra.txt', 'r') as fr:\n",
    "    \n",
    "    nmt_data = []\n",
    "    \n",
    "    for lines in fr.readlines():\n",
    "        splits = lines.split('\\t')\n",
    "        \n",
    "        i = {\n",
    "            'src': splits[1],\n",
    "            'tgt': splits[0]\n",
    "        }\n",
    "        \n",
    "        nmt_data.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save vocab_file\n",
    "\n",
    "with open('src_vocab.txt', 'w') as src:\n",
    "    el = ' '.join([a['src'] for a in nmt_data])\n",
    "    src.write(el)\n",
    "\n",
    "with open('tgt_vocab.txt', 'w') as tgt:\n",
    "    el = ' '.join([a['tgt'] for a in nmt_data])\n",
    "    tgt.write(el)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'src': \"J'ai payé en espèce.\", 'tgt': 'I paid in cash.'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmt_data[10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Setting tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tokenizers\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "\n",
    "from tokenizers import normalizers\n",
    "from tokenizers.normalizers import Lowercase, NFD, StripAccents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenizer(vocab_file, vocab_size=30000, single_format='[SOS] $A [EOS]'):\n",
    "    \n",
    "    # Instanciate a trainer\n",
    "    trainer = tokenizers.trainers.WordLevelTrainer(vocab_size=vocab_size, special_tokens=['[PAD]', '[SOS]', '[EOS]', '[UNK]'])\n",
    "    \n",
    "    # Instanciate a tokenizer\n",
    "    tokenizer = Tokenizer(WordLevel(unk_token='[UNK]'))\n",
    "    \n",
    "    # Adding pre-tokenizer\n",
    "    tokenizer.pre_tokenizer = Whitespace()\n",
    "    \n",
    "    # Adding normalizers\n",
    "    tokenizer.normalizer = normalizers.Sequence([Lowercase(), NFD(), StripAccents()])\n",
    "    \n",
    "    # Post-Processing\n",
    "    tokenizer.post_processor = TemplateProcessing(\n",
    "        single=single_format,\n",
    "        special_tokens=[\n",
    "            (\"[SOS]\", 1), \n",
    "            (\"[EOS]\", 2)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    tokenizer.train([vocab_file], trainer)\n",
    "    \n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_tokenizer = create_tokenizer('src_vocab.txt', single_format='$A')\n",
    "\n",
    "tgt_tokenizer = create_tokenizer('tgt_vocab.txt', single_format='[SOS] $A [EOS]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def switch_mode(tokenizer, max_len=200):\n",
    "    tokenizer.enable_truncation(max_len)\n",
    "    tokenizer.enable_padding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7767, 3, 8091, 21745, 4]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_tokenizer.encode(nmt_data[10000]['tgt']).ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Creating dataset and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import librairies ...\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NMTdata(Dataset):\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        \n",
    "        self.data = data\n",
    "        \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        current = self.data[idx]\n",
    "        \n",
    "        return current['src'], current['tgt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = NMTdata(nmt_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(batch):\n",
    "    \n",
    "    src = [item[0] for item in batch]\n",
    "    tgt = [item[1] for item in batch]\n",
    "    \n",
    "    switch_mode(src_tokenizer)\n",
    "    switch_mode(tgt_tokenizer)\n",
    "    \n",
    "    src = src_tokenizer.encode_batch(src)\n",
    "    src = torch.LongTensor([i.ids for i in src])\n",
    "    \n",
    "    tgt = tgt_tokenizer.encode_batch(tgt)\n",
    "    tgt = torch.LongTensor([i.ids for i in tgt])\n",
    "                \n",
    "    return [src, tgt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "185583\n"
     ]
    }
   ],
   "source": [
    "all_dataset = NMTdata(nmt_data)\n",
    "\n",
    "print(len(all_dataset))\n",
    "\n",
    "train, val = random_split(all_dataset, [len(all_dataset)-5000, 5000])\n",
    "\n",
    "train_loader = DataLoader(train, batch_size=256, shuffle=True, collate_fn=collate, num_workers=15)\n",
    "\n",
    "val_loader = DataLoader(val, batch_size=512, shuffle=False, collate_fn=collate, num_workers=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Creating Encoder Decoder model\n",
    "\n",
    "same as bahdanau et al., 2015 (https://arxiv.org/abs/1409.0473)\n",
    "\n",
    "* bi-GRU as Encoder: the formard method should return a tensor containing all the encoder's hidden states (context vectors)\n",
    "* Use/Implement attention mecanism (bahdanau, dot product, bilinear, ...)\n",
    "* GRU as Decoder: the formard method should return the predicted sequence, the last hidden state of the decoder and the attention vector at each timesteps\n",
    "\n",
    "See Notebook 014-3 for inspiration ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### import modules\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformer_utils import get_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard dot product attention\n",
    "\n",
    "def attention(query, key, value, mask=None):\n",
    "    \n",
    "    d_model = query.size(-1)\n",
    "    \n",
    "    score = query @ key.transpose(-1, -2) / np.sqrt(d_model)\n",
    "    \n",
    "    ## mask and score have to be broadcastable\n",
    "    ## For understanding array broadcasting see https://numpy.org/doc/stable/user/basics.broadcasting.html\n",
    "    if mask != None:\n",
    "        score = score.masked_fill_(mask == 0, float('-inf'))\n",
    "\n",
    "    normalized_score = torch.softmax(score, dim=-1)\n",
    "    \n",
    "    output = normalized_score @ value\n",
    "        \n",
    "    return output, normalized_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleRNN(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.rnn_1 = nn.GRU(d_model, d_model, batch_first=True, bidirectional=True, dropout=0.1)\n",
    "        self.rnn_2 = nn.GRU(d_model * 2, d_model, batch_first=True, dropout=0.1)\n",
    "        \n",
    "    def forward(self, x, seq_len):\n",
    "        \n",
    "        x = nn.utils.rnn.pack_padded_sequence(x, seq_len, batch_first=True, enforce_sorted=False)\n",
    "        \n",
    "        x, h = self.rnn_1(x)\n",
    "                \n",
    "        x, h = self.rnn_2(x)\n",
    "        \n",
    "        x, _ = nn.utils.rnn.pad_packed_sequence(x, batch_first=True)\n",
    "                \n",
    "        return x, h # [batch_size, len, dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NMTmodel(nn.Module):\n",
    "    def __init__(self, src_vocab=src_tokenizer.get_vocab_size(), tgt_vocab=tgt_tokenizer.get_vocab_size(), d_model=512):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.tgt_vocab = tgt_vocab\n",
    "        \n",
    "        self.drop = nn.Dropout(0.1)\n",
    "        \n",
    "        self.scr_embedding = nn.Embedding(src_vocab, d_model, padding_idx=0)\n",
    "        \n",
    "        self.tgt_embedding = nn.Embedding(tgt_vocab, d_model, padding_idx=0)\n",
    "                \n",
    "        self.rnn_encoder = DoubleRNN(d_model)\n",
    "        \n",
    "        self.rnn_decoder = nn.GRUCell(d_model, d_model)\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(d_model * 2, tgt_vocab)\n",
    "        )\n",
    "        \n",
    "    def encode(self, x):\n",
    "        \n",
    "        mask, _ = get_masks(x)\n",
    "        \n",
    "        x_emb = self.drop(self.scr_embedding(x))\n",
    "        \n",
    "        lens = mask.sum(-1).squeeze().cpu()\n",
    "                \n",
    "        context, h = self.rnn_encoder(x_emb, lens) # bs, len, dim * 2\n",
    "        \n",
    "        bs, slen, _ = context.size()\n",
    "        \n",
    "        return context, h, mask\n",
    "    \n",
    "    def decode_step(self, y_t, context, h_t, mask=None):\n",
    "            \n",
    "        y_t = self.drop(self.tgt_embedding(y_t))\n",
    "                \n",
    "        h_t = self.rnn_decoder(y_t, h_t)\n",
    "        \n",
    "        output, normalized_score = attention(h_t.unsqueeze(1), context, context, mask)\n",
    "        \n",
    "        pred = self.fc(output)\n",
    "        \n",
    "        return pred, normalized_score, h_t\n",
    "    \n",
    "    \n",
    "    def decode(self, y, context, h_t=None, mask=None):\n",
    "            \n",
    "        bs, slen = y.size()\n",
    "                \n",
    "        outs = []\n",
    "                \n",
    "        for t in range(slen):\n",
    "            \n",
    "            pred, normalized_score, h_t = self.decode_step(y[:, t], context, h_t, mask)\n",
    "            \n",
    "            outs.append(pred)\n",
    "        \n",
    "        pred = torch.cat(outs, dim=1)\n",
    "        \n",
    "        return pred\n",
    "            \n",
    "            \n",
    "    def teacher_forcing_enc_dec(self, x, y):\n",
    "        \n",
    "        context, h, mask = self.encode(x)\n",
    "        \n",
    "        outs = self.decode(y, context, h.squeeze(0), mask=mask)\n",
    "        \n",
    "        return outs\n",
    "        \n",
    "    def forward(self, src, tgt):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def compute_loss(self, x, y):\n",
    "        \n",
    "        pred = self.teacher_forcing_enc_dec(x, y[:, :-1])\n",
    "        \n",
    "        y = y[:, 1:]\n",
    "        \n",
    "        y = y.reshape(-1)\n",
    "        \n",
    "        pred = pred.view(-1, self.tgt_vocab)\n",
    "        \n",
    "        loss = F.cross_entropy(pred, y, ignore_index=0)\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NMTmodel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in train_loader:\n",
    "    y = model.compute_loss(x, y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train_one_epoch(net: nn.Module, opt: torch.optim, dataloader: torch.utils.data.DataLoader):\n",
    "    \n",
    "    net.train()\n",
    "    \n",
    "    for param in net.parameters():\n",
    "        device = param.device\n",
    "        break\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    pbar = tqdm(dataloader)\n",
    "    \n",
    "    for x, y in pbar:\n",
    "\n",
    "        net.zero_grad()\n",
    "\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        loss = net.compute_loss(x, y)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        opt.step()\n",
    "        \n",
    "        loss_item = loss.item()\n",
    "        \n",
    "        losses.append(loss_item)\n",
    "        \n",
    "        pbar.set_description(f'train_loss = {np.array(losses).mean()}')\n",
    "        \n",
    "    return np.array(losses).mean()\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(net: nn.Module, dataloader: torch.utils.data.DataLoader):\n",
    "    \n",
    "    net.eval()\n",
    "    \n",
    "    for param in net.parameters():\n",
    "        device = param.device\n",
    "        break\n",
    "     \n",
    "    losses = []\n",
    "    \n",
    "    for x, y in dataloader:\n",
    "\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        loss = net.compute_loss(x, y)\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "                    \n",
    "    return np.array(losses).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NMTmodel().cuda()\n",
    "\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/706 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.590476989746094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_loss = 4.401522872130526: 100%|██████████| 706/706 [03:15<00:00,  3.61it/s] \n",
      "  0%|          | 0/706 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.393892455101013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_loss = 3.120895153064566: 100%|██████████| 706/706 [07:44<00:00,  1.52it/s] \n",
      "  0%|          | 0/706 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8132535457611083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_loss = 2.6220931794082816: 100%|██████████| 706/706 [08:44<00:00,  1.34it/s]\n",
      "  0%|          | 0/706 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4075923919677735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_loss = 2.2522229010255073: 100%|██████████| 706/706 [08:37<00:00,  1.36it/s]\n",
      "  0%|          | 0/706 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.113707661628723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_loss = 1.9578542437499373: 100%|██████████| 706/706 [09:30<00:00,  1.24it/s]\n",
      "  0%|          | 0/706 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8759214520454406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_loss = 1.7138013561116399: 100%|██████████| 706/706 [09:33<00:00,  1.23it/s]\n",
      "  0%|          | 0/706 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6932036995887756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_loss = 1.5162221044724116: 100%|██████████| 706/706 [09:02<00:00,  1.30it/s]\n",
      "  0%|          | 0/706 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.549120008945465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_loss = 1.3576835015677866: 100%|██████████| 706/706 [08:29<00:00,  1.38it/s]\n",
      "  0%|          | 0/706 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.449587070941925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_loss = 1.2284283663328242: 100%|██████████| 706/706 [09:14<00:00,  1.27it/s]\n",
      "  0%|          | 0/706 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3728409647941588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_loss = 1.1214582285192127: 100%|██████████| 706/706 [09:32<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3114278316497803\n"
     ]
    }
   ],
   "source": [
    "model.cuda()\n",
    "\n",
    "for i in range(10):\n",
    "    \n",
    "    if i==0:\n",
    "        print(validate(model, val_loader))\n",
    "        \n",
    "    train_one_epoch(model, opt, train_loader)\n",
    "    \n",
    "    print(validate(model, val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "# go to cpu for prediction mode\n",
    "\n",
    "model.cpu()\n",
    "model.eval()\n",
    "\n",
    "print('ok')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Prediction and attention visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "## see notebook 014-3 for inspiration\n",
    "\n",
    "@torch.no_grad()\n",
    "def translate(french):\n",
    "    \n",
    "    src = torch.LongTensor(src_tokenizer.encode(french).ids).unsqueeze(0)\n",
    "        \n",
    "    y = torch.LongTensor([1])\n",
    "    \n",
    "    lens = torch.LongTensor([src.size()[1]])\n",
    "        \n",
    "    context = model.scr_embedding(src)\n",
    "        \n",
    "    context, h_t = model.rnn_encoder(context, lens)\n",
    "    \n",
    "    print(src.shape)\n",
    "        \n",
    "    sequences = []\n",
    "    att = []\n",
    "    \n",
    "    while True and len(sequences) < 20:\n",
    "            \n",
    "        pred, normalized_score, h_t = model.decode_step(y, context, h_t.squeeze(1))\n",
    "                \n",
    "        pred = torch.argmax(pred, -1).item()\n",
    "        \n",
    "        if pred == 2: # stop when eos\n",
    "            break\n",
    "            \n",
    "        att.append(normalized_score)\n",
    "        \n",
    "        sequences.append(pred)\n",
    "        \n",
    "        y = torch.LongTensor([pred])\n",
    "        \n",
    "    att = torch.cat(att, dim=1)\n",
    "    \n",
    "    print(att.shape)\n",
    "            \n",
    "    return tgt_tokenizer.decode(sequences), att.squeeze().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Fais court.'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmt_data[val.indices[0]]['src']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source: Je pensais que vous seriez rentrées chez vous.\n",
      "target: I thought you'd gone home.\n",
      "torch.Size([1, 9])\n",
      "torch.Size([1, 8, 9])\n",
      "pred: i thought you ' d be home .\n"
     ]
    }
   ],
   "source": [
    "example = nmt_data[val.indices[np.random.randint(0, 5000)]]\n",
    "\n",
    "print('source:', example['src'])\n",
    "\n",
    "print('target:', example['tgt'])\n",
    "\n",
    "pred, att = translate(example['src'])\n",
    "\n",
    "print('pred:', pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(att, cmap='Greys', yticklabels=pred.split(' '), xticklabels=src_tokenizer.encode(example['src']).tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7 (bonus): Evalute the model performance using BLEU metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37_UZA",
   "language": "python",
   "name": "py37_uza"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
